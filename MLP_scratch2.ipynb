{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Augmentation\n",
    "1)Rotation\n",
    "2)Noise\n",
    "3)Horizontal Flip\n",
    "'''\n",
    "\n",
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "    return image_array[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.weights = np.zeros(shape=(input.shape[1], 10))\n",
    "        bias = np.zeros(shape=(10,))\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = np.matmul(input, self.weights) + bias\n",
    "        return output\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return np.matmul(input, self.weights) + self.biases\n",
    "      \n",
    "    def backward(self,input,grad_output):\n",
    "        grad_input = np.dot(grad_output,np.transpose(self.weights))#Gradient wrt to input\n",
    "        grad_weights = np.transpose(np.dot(np.transpose(grad_output),input))#Gradient w.r.t. weights \n",
    "        grad_biases = np.sum(grad_output, axis = 0)#Gradient w.r.t. biases\n",
    "        #Weight updation\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        return grad_input\n",
    "\n",
    "'''\n",
    "Activation Functions\n",
    "\n",
    "1)Sigmoid\n",
    "2)Relu\n",
    "3)LRelu\n",
    "4)Tanhh\n",
    "5)Linear\n",
    "\n",
    "'''   \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        '''ReLU layer simply applies elementwise rectified linear unit to all inputs'''\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''Apply elementwise ReLU to [batch, input_units] matrix'''\n",
    "        self.input = input\n",
    "        self.output = (1 / (1 + np.exp(-input)))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        '''Compute gradient of loss w.r.t. ReLU input'''\n",
    "        sigmoid_grad = (1 - self.output) * self.output\n",
    "        return (grad_output * sigmoid_grad)\n",
    "    \n",
    "    \n",
    "class LReLU(Layer):\n",
    "    def __init__(self):\n",
    "        '''ReLU layer simply applies elementwise rectified linear unit to all inputs'''\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''Apply elementwise ReLU to [batch, input_units] matrix'''\n",
    "        return np.maximum(-input,input)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        '''Compute gradient of loss w.r.t. ReLU input'''\n",
    "        lrelu_grad = []\n",
    "#         print(input.shape)\n",
    "        for i in input:\n",
    "            lrelu_grad1=[]\n",
    "            for j in i:\n",
    "                lrelu_grad1.append(1 if j > 0 else -1)\n",
    "            lrelu_grad.append(lrelu_grad1)\n",
    "        lrelu_grad = np.array(lrelu_grad)\n",
    "#         print(grad_output.shape,lrelu_grad.shape)\n",
    "        return grad_output*lrelu_grad \n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.maximum(0,input)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad \n",
    "\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = (np.exp(input) - np.exp(-input))/(np.exp(input) + np.exp(-input))\n",
    "        return self.output\n",
    "    def backward(self,input,grad_output):\n",
    "        return (1 - np.square(self.output))*grad_output\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    def backward(self,input,grad_output):\n",
    "        return 1 * grad_output\n",
    "        \n",
    "class softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = np.exp(predicted) / np.exp(predicted).sum(axis=-1,keepdims=True)\n",
    "        return self.output\n",
    "    def backward(self,input,grad_output):\n",
    "        return self.output*(1-self.output)*grad_output\n",
    "        \n",
    "''' Loss Functions and their gradients \n",
    "\n",
    "1) Logloss\n",
    "2) CrossEntropy\n",
    "\n",
    "'''\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''\n",
    "Here if we use argmax in the predict function we cant use logloss\n",
    "So we have to implement another kind of soft max instaed of argmax\n",
    "'''\n",
    "def logloss_with_predicted(predicted,reference_answers):\n",
    "    reference_answers = to_categorical(list(reference_answers),num_classes=10)\n",
    "    reference_answers = np.array(reference_answers)\n",
    "#     print(predicted.shape,reference_answers.shape)\n",
    "#     print(predicted[0],reference_answers[0])\n",
    "    loss = reference_answers * np.log(predicted) + (1-reference_answers) * np.log(1-predicted)\n",
    "    return loss    \n",
    "\n",
    "def grad_logloss_with_predicted(predicted,reference_answers):\n",
    "    reference_answers = to_categorical(list(reference_answers),num_classes=10)\n",
    "    reference_answers = np.array(reference_answers)\n",
    "#     print(predicted.shape,reference_answers.shape)\n",
    "#     print(predicted[0],reference_answers[0])\n",
    "    return (predicted - reference_answers)\n",
    "    \n",
    "def softmax_crossentropy_with_predicted(predicted,reference_answers):\n",
    "    predicted_for_answers = predicted[np.arange(len(predicted)),reference_answers]\n",
    "#     print(reference_answers[0])\n",
    "    xentropy = - predicted_for_answers + np.log(np.sum(np.exp(predicted),axis=-1))\n",
    "#     print(xentropy)\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_predicted(predicted,reference_answers):\n",
    "    ones_for_answers = np.zeros_like(predicted)\n",
    "    ones_for_answers[np.arange(len(predicted)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(predicted) / np.exp(predicted).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / predicted.shape[0]\n",
    "  \n",
    "\n",
    "\n",
    "''' Load the mnist data set '''\n",
    "\n",
    "import keras\n",
    "def load_dataset(flatten=True,augment = True):\n",
    "    if augment==False:\n",
    "        (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "        # normalize x\n",
    "        X_train = X_train.astype(float) / 255.\n",
    "        X_test = X_test.astype(float) / 255.\n",
    "\n",
    "        # we reserve the last 10000 training examples for validation\n",
    "        X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "        y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "        if flatten:\n",
    "            X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "            X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "            X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    else:\n",
    "        (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "        X_train = X_train.astype(float) / 255.\n",
    "        X_test = X_test.astype(float) / 255.\n",
    "        \n",
    "        idx = random.sample(range(0,len(X_train)),60)\n",
    "#         print(random_rotation(np.array(X_train[idx[:20],:])).shape)\n",
    "        X_train = np.append(X_train,random_rotation(np.array(X_train[idx[:20],:])),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idx[:20]],axis=0)\n",
    "        X_train = np.append(X_train,random_noise(np.array(X_train[idx[20:40],:])),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idx[20:40]],axis=0)\n",
    "        X_train = np.append(X_train,random_rotation(np.array(X_train[idx[40:60],:])),axis=0)\n",
    "        y_train = np.append(y_train,y_train[idx[40:60]],axis=0)\n",
    "        X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "        y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "        if flatten:\n",
    "            X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "            X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "            X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n",
    "    \n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],100))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(100,200))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(200,50))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(50,10))\n",
    "\n",
    "def forward(network, X):\n",
    "    \n",
    "    activations = []\n",
    "    input = X\n",
    "    for i in range(len(network)):\n",
    "        activations.append(network[i].forward(X))\n",
    "        X = network[i].forward(X)\n",
    "        \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network,X,conf=False):\n",
    "    \n",
    "    predicted = forward(network,X)[-1]\n",
    "    '''\n",
    "    WITHOUT CONFIDENCE SCORES\n",
    "    '''\n",
    "    if conf == False:\n",
    "        return predicted.argmax(axis=-1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def train(network,X,y,regularize='L1Norm',alp=0.01):\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    predicted = layer_activations[-1]\n",
    "#     alp = int(input('Enter alpha for regularizer'))\n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_predicted(predicted,y)\n",
    "    \n",
    "#     loss = logloss_with_predicted(predicted,y)\n",
    "#     print(loss.shape)\n",
    "    '''\n",
    "    Regularizer\n",
    "    1)L1Norm\n",
    "    2)L2Norm\n",
    "    '''\n",
    "    regularizer = 0\n",
    "    if regularize == 'L1Norm':\n",
    "        for i in range(0,len(network),2):\n",
    "            regularizer += abs(np.mean(network[i].weights))\n",
    "    elif regularize=='L2Norm':\n",
    "        for i in range(0,len(network),2):\n",
    "            regularizer += (np.mean(network[i].weights) ** 2)\n",
    "    loss += alp * regularizer\n",
    "    loss_grad = grad_softmax_crossentropy_with_predicted(predicted,y)\n",
    "#     loss_grad = grad_logloss_with_predicted(predicted,y)\n",
    "#     print('1',loss_grad.shape)\n",
    "    for i in range(1, len(network)):\n",
    "        loss_grad = network[len(network) - i].backward(layer_activations[len(network) - i - 1], loss_grad)\n",
    "    \n",
    "    return np.mean(loss)\n",
    "  \n",
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "train_log = []\n",
    "val_log = []\n",
    "for epoch in range(10):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "#     clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
